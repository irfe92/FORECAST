---
title: "Regression penalisee"
author: "Nhu-Nguyen"
date: "2 avril 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



REGRESSION PENALISEE RIDGE
```{r}
library(glmnet)
# library(caTools)
# library(randomForest)
library(ggplot2)

# definition variables Y et X
library(questionr)
don <- rename.variable(don, "conso", "Y")
Y=don$Y


# par defaut, glmnet standardise les variables pour les mettre sur la même échelle.
# si on ne veut pas, standardise=FALSE
X=model.matrix(Y~.,don)[,-1]
dim(X) # en colonne toutes les variables yc dummy variables


# grille de lambda de 10^10 à 10^-2
# pour mémoire, si lambda=0, c'est le MCO
grid=10^seq(10,-2,length=100)


# REGRESSION PENALISEE RIDGE sur une grille de lambda
# Ridge pour alpha=0 et Lasso pour alpha=1
ridge.mod=glmnet(X,Y,alpha=0,lambda=grid) 

# a chaque valeur de lambda est associé un vecteur de coefficients de regression stockés dans une matrice coef()
# nombre de lignes = nombre de variables + l'intercept
# plus lambda est grand et plus les coefficients sont petits
dim(coef(ridge.mod))


# PREDICT AVEC TRAIN ET TEST
set.seed(1)
train=sample(1:nrow(X),2*nrow(X)/3)
test=(-train)
Y.test=Y[test]

ridge.mod=glmnet(X[train,],Y[train],alpha=0,lambda=grid)
ridge.pred=predict(ridge.mod,newx=X[test,]) 
# ridge.pred=predict(ridge.mod,s=4, newx=X[test,]) 
# s est la valeur du lambda pour la prédiction. Par défaut, c'est la séquence de lambda qui a été utilisée pour creer le modèle
MSE=mean((ridge.pred-Y.test)^2)
MSE


# CROSS VALIDATION HOLD OUT POUR LE CHOIX DU LAMBDA DANS RIDGE
# recherche de lambda qui minimise MSE
grid=10^seq(10,-2,length=100) 
mse.lambda=rep(NA,length(grid))
for(i in 1:length(grid)) {
ridge.mod=glmnet(X[train,],Y[train],alpha=0,lambda=grid[i])
ridge.pred=predict(ridge.mod,s=grid[i], newx=X[test,]) 
mse.lambda[i] <- mean((Y.test-ridge.pred)^2)
  }
plot(mse.lambda) # visuellement à partir de la 20ème valeur

diff.lambda=rep(NA,(length(mse.lambda)-1)) # calcul de la variation de MSE d'une valeur lambda à l'autre
for (i in 1:length(mse.lambda)) { 
diff.lambda[i]=mse.lambda[i+1]-mse.lambda[i]
}
plot(diff.lambda) # la variation de mse.lambda baisse jusqu'à un certain point puis réaugmente jusqu'à la fin

diff.min<-which.min(diff.lambda) # recherche de la plus petite variation 
diff.min # c'est la 12ème variation qui est la plus petite.A partir de cette valeur, la variation réaugmente
lambda.min=grid[diff.min]
lambda.min

# MODELISATION AVEC LAMBDA.MIN TROUVEE PAR CROSS VALIDATION

# train and test avec sample
set.seed(1)
train=sample(1:nrow(X),2*nrow(X)/3)
test=(-train)
Y.test=Y[test]
# X=model.matrix(Y~.,don)[,-1]


ridge.mod.best=glmnet(X[train,],Y[train],alpha=0,lambda=lambda.min)
ridge.pred=predict(ridge.mod.best,s=lambda.min, newx=X[test,]) 
# s est la valeur du lambda pour la prédiction. Par défaut, c'est la séquence de lambda qui a été utilisée pour creer le modèle
MSE.lambda.min=mean((ridge.pred-Y.test)^2)
MSE.lambda.min



# plot des valeurs prédites vs valeurs réelles # pas pertinent car les prédictions sont écrasées par Ridge
date=data.frame(X)$Date

ggplot() +
  geom_line(aes(x = date[test], y = Y[test]),
             colour = 'red') +
  geom_line(aes(x = date[test], y = ridge.pred),
            colour = 'blue') +
  ggtitle('Regression Ridge, en bleu prédiction') +
  xlab('date') +
  ylab('conso')



```


REGRESSION PENALISEE LASSO
```{r}

library(glmnet)
# library(caTools)
# library(randomForest)
library(ggplot2)

# definition variables Y et X
library(questionr)
don <- rename.variable(don, "conso", "Y")
Y=don$Y


# par defaut, glmnet standardise les variables pour les mettre sur la même échelle.
# si on ne veut pas, standardise=FALSE
X=model.matrix(Y~.,don)[,-1]
# dim(X) # en colonne toutes les variables yc dummy variables


# grille de lambda de 10^10 à 10^-2
# pour mémoire, si lambda=0, c'est le MCO
grid=10^seq(10,-2,length=100)


# REGRESSION PENALISEE RIDGE sur une grille de lambda
# Ridge pour alpha=0 et Lasso pour alpha=1
lasso.mod=glmnet(X,Y,alpha=1,lambda=grid) 

# a chaque valeur de lambda est associé un vecteur de coefficients de regression stockés dans une matrice coef()
# nombre de lignes = nombre de variables + l'intercept
# plus lambda est grand et plus les coefficients sont petits
# dim(coef(lasso.mod))


# PREDICT AVEC TRAIN ET TEST
set.seed(1)
train=sample(1:nrow(X),2*nrow(X)/3)
test=(-train)
Y.test=Y[test]

lasso.mod=glmnet(X[train,],Y[train],alpha=1,lambda=grid) # Ridge pour alpha=0 et Lasso pour alpha=1
lasso.pred=predict(lasso.mod,newx=X[test,]) 
# lasso.pred=predict(lasso.mod,s=4, newx=X[test,]) 
# s est la valeur du lambda pour la prédiction. Par défaut, c'est la séquence de lambda qui a été utilisée pour creer le modèle
MSE=mean((lasso.pred-Y.test)^2)
MSE


# CROSS VALIDATION HOLD OUT POUR LE CHOIX DU LAMBDA DANS lasso
# recherche de lambda qui minimise MSE
grid=10^seq(10,-2,length=100) 
mse.lambda=rep(NA,length(grid))
for(i in 1:length(grid)) {
lasso.mod=glmnet(X[train,],Y[train],alpha=1,lambda=grid[i])
lasso.pred=predict(lasso.mod,s=grid[i], newx=X[test,]) 
mse.lambda[i] <- mean((Y.test-lasso.pred)^2)
  }
plot(mse.lambda) # visuellement avant la 20ème valeur

diff.lambda=rep(NA,(length(mse.lambda)-1)) # calcul de la variation de MSE d'une valeur lambda à l'autre
for (i in 1:length(mse.lambda)) { 
diff.lambda[i]=mse.lambda[i+1]-mse.lambda[i]
}
plot(diff.lambda) # la variation de mse.lambda baisse jusqu'à un certain point puis réaugmente jusqu'à la fin

diff.min<-which.min(diff.lambda) # recherche de la plus petite variation 
diff.min # c'est la 17ème variation qui est la plus petite.A partir de cette valeur, la variation réaugmente
lambda.min=grid[diff.min]
lambda.min

# MODELISATION AVEC LAMBDA.MIN TROUVEE PAR CROSS VALIDATION

# train and test avec sample
set.seed(1)
train=sample(1:nrow(X),2*nrow(X)/3)
test=(-train)
Y.test=Y[test]
# X=model.matrix(Y~.,don)[,-1]


lasso.mod.best=glmnet(X[train,],Y[train],alpha=1,lambda=lambda.min)
lasso.pred=predict(lasso.mod.best,s=lambda.min, newx=X[test,]) 
# s est la valeur du lambda pour la prédiction. Par défaut, c'est la séquence de lambda qui a été utilisée pour creer le modèle
MSE.lambda.min=mean((lasso.pred-Y.test)^2)
MSE.lambda.min



# plot des valeurs prédites vs valeurs réelles # pas pertinent car les prédictions sont écrasées par lasso
date=data.frame(X)$Date

ggplot() +
  geom_line(aes(x = date[test], y = Y[test]),
             colour = 'red') +
  geom_line(aes(x = date[test], y = lasso.pred),
            colour = 'blue') +
  ggtitle('Regression lasso, en bleu prédiction') +
  xlab('date') +
  ylab('conso')


```



```{r}

```

