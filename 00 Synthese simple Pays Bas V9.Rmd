---
title: "Synthese Pays Bas simplifiee"
author: "Nhu-Nguyen Ngo"
date: "16 avril 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Script autosuffisant pour les modeles retenus pour Pays Bas, avec toutes les variables d'environnement.

Pour generaliser aux autres pays:
remplacer nom fichier
remplacer title
remplacer base: base_NL_F
remplacer suffixe: _NL
remplacer nom: Pays Bas


# PACKAGES
```{r}

# DONNEES VISUALISATION
library(stargazer)
library(ggplot2)
library(questionr)
library(dplyr)
library(lubridate)      # pour les dates
library(dummies)        # creationn de variables dummies (pour bestglm)
library(forecast)       # plot sympa des residus
library(corrplot)       # plot de la matrice de correlation
library(PerformanceAnalytics)


# TREE
library(rpart)				  # Popular decision tree algorithm
library(rattle)					# Fancy tree plot
library(rpart.plot)			# Enhanced tree plots
library(RColorBrewer)		# Color selection for fancy tree plot
library(party)					# Alternative decision tree algorithm
library(partykit)				# Convert rpart object to BinaryTree


# selection de variable
library(bestglm)
library(leaps)          # regsubset


# cross validation
library(stats)          # fonction glm
library(caret)

# CLUSTERING
library(cluster)
library(fastcluster)
library(ade4)
library(FactoMineR)



# MODELES
library(MASS) # LDA
library(ISLR)
library(glmnet)         # Poly, GAM
library(boot)           # boostraping
library(splines)
library(caTools)
library(randomForest)
library(e1071)          # SVR
library(nnet)           # reseau neurones
library(neuralnet)
library(mlbench)
library(gbm)
library(xgboost)

# paralellisation
library(doParallel)
library(foreach)


```



# BASE DE DONNEES ET FORMATAGE VARIABLES TRAIN TEST 
```{r}

# sur la base centree reduite, sans les autres variables meteo
don <- base_NL_F_cr
dim(don)

# suppression des variales liees a la temperature teff, seuil, T00
var_delete <- c(which(colnames(don)== "teff"),which(colnames(don)== "seuil"),which(colnames(don)== "T00"))
don<-don[,-var_delete] 
head(don)

# creation des variables Y (variable cible)
don<- rename.variable(don, "Conso", "Y")
head(don)


# Creation de l'echantillon train 80 et test 20
set.seed(1)
dim<-nrow(don)
split=0.8
train=sample(dim,split*dim,replace=FALSE)# vecteur d'entier pour la population d'individus en base d'apprentissage

don.train_NL=don[train,]
don.test_NL=don[-train,]

# variables pour modeles polynomial et splines
Y=don$Y 
Y.train_NL=Y[train]
Y.test_NL=Y[-train]

# matrice avec que les variables predictives
donX=don[, which(colnames(don)!="Y")]
donX.train_NL=donX[train,]
donX.test_NL=donX[-train,]

# model matrice sur base de test
test_NL=model.matrix(Y~.,data=don[-train,])



```


# fonction mape Mean Absolute Percentage Error
MAPE ne semble pas adapte pour les donnees centrees reduites, car le denominateur peut être souvent proche de zero. Il faudrait plutot utiliser le MASE (mean absolute scaled error). Mais accuracy ne reconnaît pas les modeles random forest , SVR, NN, XGB

```{r}
# Y variable cible
# model = fit du model
# don.test_NL : base de test


fun_mse <- function (Y.test_NL, model, don.test_NL) {
  mse <- mean((Y.test_NL-predict(model,don.test_NL))^2)
  return(mse)
}




fun_mape = function (Y,model, don.test_NL) {
  error <- Y-predict(model,don.test_NL)
  mape <- mean(abs(error/Y))*100
  return(mape)
}




fun_mase = function (model) {
  acc <- accuracy(model)
  MASE <- acc [[6]]
  return(MASE)
}


```


# MODELES OLS
```{r}

# SANS INTERACTION ============================================================================================================================

# modele lineaire entre Y  et Temp et les variables month, year, day
RL_NL <- lm(Y~  Temp + month + year + day, data=don.train_NL)
pred_RL_NL=predict(RL_NL, newdata=don.test_NL, se=T)
MSE_RL_NL= mean((Y.test_NL-predict(RL_NL,don.test_NL))^2)
# MAPE_RL_NL = fun_mape(Y.test_NL,RL_NL, don.test_NL)


# residus
plot(RL_NL)
checkresiduals(RL_NL)

# AVEC INTERACTION ============================================================================================================================


# modele lineaire avec interaction entre Temp et les variables month, year, day ----------------------------------------------------------------
RLI_NL <- lm(Y~(month + year + day)*Temp ,data=don.train_NL)
pred_RLI_NL=predict(RLI_NL, newdata=don.test_NL, se=T)
MSE_RLI_NL= mean((Y.test_NL-predict(RLI_NL,don.test_NL))^2)
# MAPE_RLI_NL = fun_mape(Y.test_NL,RLI_NL, don.test_NL)

# residus
plot(RLI_NL)
checkresiduals(RLI_NL)


# modele lineaire avec interaction multiples entre les variables (Temp  et ses lags t1 a t7) et les variables month, year, day ------------------
RLI_NL_multi <- lm(Y~(month + year + day)* (Temp + t1 + t2 + t3 + t4 + t5 + t6 + t7), data=don.train_NL) 
pred_RLI_NL_multi=predict(RLI_NL_multi, newdata=don.test_NL, se=T)
MSE_RLI_NL_multi= mean((Y.test_NL-predict(RLI_NL_multi,don.test_NL))^2)
# MAPE_RLI_NL_multi = fun_mape(Y.test_NL,RLI_NL_multi, don.test_NL)

# residus
plot(RLI_NL_multi)
checkresiduals(RLI_NL_multi)


# modele lineaire avec interaction entre poly(Temp,2) et les autres variables ----------------------------------------------------------------
RLI_NL_P2<-lm(Y~ (month + year + day)*I(poly(Temp, 2)), data=don.train_NL) 
pred_RLI_NL_P2=predict(RLI_NL_P2, newdata=don.test_NL, se=T)
MSE_RLI_NL_P2= mean((Y.test_NL-predict(RLI_NL_P2,don.test_NL))^2)
# MAPE_RLI_NL_P2 = fun_mape(Y.test_NL,RLI_NL_P2, don.test_NL)

# residus
plot(RLI_NL_P2)
checkresiduals(RLI_NL_P2)

```



# MODELE POLYNOMIAL SIMPLE
Conso en fonction d'un polynome de Temp
```{r}

# choix du degre du polynome choisi par CV (hold out et k_fold) en minimisant le MSE
d=20 # degre max de polynome a tester
err_poly_NL=rep(NA,d)
for(i in 1:d) {
  model <- lm(formula=Y~poly(Temp,i, raw=T), data=don.train_NL)
  err_poly_NL[i] <- mean((Y.test_NL-predict(model,don.test_NL))^2)
  }

# plot(err_poly_NL,ylab="MSE", main=' MSE Pays Bas selon le degre de polynome',pch=19,type='b')

poly_NL_deg=which.min(err_poly_NL) 
poly_NL_deg 


# MODELE RETENU
POLY_NL<- lm(formula=Y~poly(Temp,poly_NL_deg, raw=T), data=don.train_NL)
pred_POLY_NL=predict(POLY_NL, newdata=don.test_NL, se=T) 
MSE_POLY_NL= mean((Y.test_NL-predict(POLY_NL,don.test_NL))^2)
# MAPE_POLY_NL = fun_mape(Y.test_NL,POLY_NL, don.test_NL)

# residus
plot(POLY_NL)
checkresiduals(POLY_NL)

```



# MODELES SPLINES SIMPLE
Conso en fonction d'un spline de Temp
```{r}
# pour natural spline CV pour choisir le degre de liberte (donc le nombre de noeuds) qui minimise le MSE
DF=15 # df max a tester
MSE_SP_NL_CV=rep(0,DF)
for(i in 1:DF) {
  model <- lm(Y~ns(Temp,df=i), data=don.train_NL)
  MSE_SP_NL_CV[i] <- mean((Y.test_NL-predict(model,don.test_NL))^2)
  }

# plot(sqrt(MSE_SP_NL_CV),ylab="MSE", main=' MSE Pays Bas selon le degre de liberte du spline',pch=19,type='b')

# On choisit le modele qui a la MSE la plus petite sur le test set
SP_NL_df=which.min(MSE_SP_NL_CV)
SP_NL_df 

attr(ns(don$Temp,df=SP_NL_df),"knots") #  noeuds

# modele retenu: natural spline
SP_NL <- lm ( Y~ ns(Temp, df = SP_NL_df), data=don.train_NL)
pred_SP_NL=predict(SP_NL, newdata=don.test_NL, se=T)
MSE_SP_NL= mean((Y.test_NL-predict(SP_NL,don.test_NL))^2)
# MAPE_SP_NL = fun_mape(Y.test_NL,SP_NL, don.test_NL)

# residus
plot(SP_NL)
checkresiduals(SP_NL)

```



# MODELE GAM SPLINE
Conso en fonction d'un spline sur Temp et la somme des variables month, year, day
```{r}
# pour natural spline, recherche degre df qui minimise le MSE
DF=15 # df max a tester
MSE_GAM_NL_SP_CV=rep(0,DF)
for(i in 1:DF) {
  model <- lm(Y~ns(Temp,df=i) + month + year + day, data=don.train_NL)
  MSE_GAM_NL_SP_CV[i] <- mean((Y.test_NL-predict(model,don.test_NL))^2)
  }

# Plot(MSE_GAM_NL_SP_CV, ylab="MSE", main=' MSE Pays Bas selon le degre de liberte du spline',pch=19,type='b')

# On choisit le DF qui a la MSE la plus petite sur le test set
GAM_NL_SP_df=which.min(MSE_GAM_NL_SP_CV)
GAM_NL_SP_df # 
attr(ns(don$Temp,df=GAM_NL_SP_df),"knots") #  noeuds 

# modele GAM retenu par MSe
GAM_NL_SP=lm(Y~ns(Temp,df=GAM_NL_SP_df) + month + year + day, data=don.train_NL)
pred_GAM_NL_SP=predict(GAM_NL_SP, newdata=don.test_NL, se=T)
MSE_GAM_NL_SP= mean((Y.test_NL-predict(GAM_NL_SP,don.test_NL))^2)
# MAPE_GAM_NL_SP = fun_mape(Y.test_NL,GAM_NL_SP, don.test_NL)

# residus
plot(GAM_NL_SP)
checkresiduals(GAM_NL_SP)

```



# MODELE RANDOM FOREST
avec mtry, ntree et nodesize choisis par CV hold out
```{r}

# CHOIX DE MTRY PAR CV -----------------------------------------------------------------------------------
set.seed(1)
m=15 # mtry max a tester. 
MSE_RF_NL_mtry=rep(0,m)
for(i in 1:m) {
  set.seed(1)
  model <- randomForest(Y~., data=don.train_NL, mtry = i)
  MSE_RF_NL_mtry[i] <- mean((Y.test_NL-predict(model,don.test_NL))^2)
  }

RF_NL_mtry= which.min(MSE_RF_NL_mtry) 
RF_NL_mtry  
# plot(MSE_RF_NL_mtry, xlab="mtry", ylab="MSE", main="MSE selon mtry", type="b")



# CHOIX DE NTREE PAR CV  -----------------------------------------------------------------------------------
Ntree=seq(100,1000,by=100)  # ntree a tester
d=length(Ntree)
nb=1 # nombre de tests de cross validation
MSE_RF_NL_tree=rep(NA,d*nb)
res_ntree=rep(NA,nb)   # resultat de la CV, ntree qui minimise la MSE

for (j in 1:nb) {

  for(i in 1:d) {
    set.seed(1)
    model <- randomForest(Y~., data=don.train_NL, mtry = RF_NL_mtry, ntree=Ntree[i])
    MSE_RF_NL_tree[i+j-1] <- mean((don.test_NL$Y-predict(model,don.test_NL))^2)
  }

  res_ntree[j]=Ntree[which.min(MSE_RF_NL_tree)]

}

RF_NL_ntree = Ntree[which.min(MSE_RF_NL_tree)] 
RF_NL_ntree
# barplot(MSE_RF_NL_tree, xlab="ntree", ylab="MSE", ylim = range(MSE_RF_NL_tree), main="MSE selon ntree")



# CHOIX DE NODESIZE PAR CV  -----------------------------------------------------------------------------------
n_list=seq(from=1,to=10,by=1)  # nodesize a tester
d=length(n_list)
nb=1 # nombre de tests de cross validation
MSE_RF_NL_node=rep(NA,d*nb)
res_node=rep(NA,nb)   # resultat de la CV qui minimise la MSE

for (j in 1:nb) {
  
  for(i in 1:d) {
    set.seed(1)
    model <- randomForest(Y~., data=don.train_NL,mtry = which.min(MSE_RF_NL_mtry), ntree=RF_NL_ntree, nodesize = n_list[i])
    MSE_RF_NL_node[i+j-1] <- mean((Y.test_NL-predict(model,don.test_NL))^2)
    names(MSE_RF_NL_node)[i] <- paste(as.character(n_list[i]),"node",sep="_")
  }

  res_node[j]=n_list[which.min(MSE_RF_NL_node)]

}

RF_NL_node = which.min(MSE_RF_NL_node)
RF_NL_node 
# barplot(MSE_RF_NL_node, xlab="node", ylab="MSE", ylim = range(MSE_RF_NL_node) , names = names(MSE_RF_NL_node) ,main="MSE selon nodesize",las=0)



# MODELE FINAL  ----------------------------------------------------------------------------------------------
RF_NL<-randomForest(Y~., mtry = RF_NL_mtry, ntree= RF_NL_ntree, nodesize = RF_NL_node ,data=don.train_NL)
pred_RF_NL = predict(RF_NL, don.test_NL)
MSE_RF_NL= mean((Y.test_NL-predict(RF_NL,don.test_NL))^2)
# MAPE_RF_NL = fun_mape(Y.test_NL,RF_NL, don.test_NL)

# # residus : la saisonnalite n'a pas ete bien captee: le graphe des residus est sinusoidal
res_RF_NL=don.test_NL$Y-RF_NL$predicted
checkresiduals(res_RF_NL)

# # importances des variables
# # plot
# varImpPlot(RF_NL_fin)
# 
# # liste variable par importance
# RF_NL_fin$importance
# 
# # liste variable par importance ordonnee
# RF_NL_fin$importance[order(RF_NL_fin$importance[, 1], decreasing = TRUE), ]



```



MODELE SVR
```{r}
# modele
SVR_NL = svm(Y~.,don.train_NL)
pred_SVR_NL = predict(SVR_NL, don.test_NL)
MSE_SVR_NL= mean((Y.test_NL-predict(SVR_NL,don.test_NL))^2)
# MAPE_SVR_NL = fun_mape(Y.test_NL,SVR_NL, don.test_NL)

# residus
# plot(SVR_NL_tot$residuals)
res_SVR_NL=don.test_NL$Y-pred_SVR_NL
checkresiduals(res_SVR_NL) 



```



# MODELE RESEAUX DE NEURONES
```{r}

# choix des parametres size et decay par CV avec caret
controlList <- trainControl(method = "cv", number = 5)
tuneMatrix <- expand.grid(size = c(1, 2, 3, 4, 5, 6), decay = seq(from = 0, to = 0.5, by=0.1))

set.seed(1)
NN_NL_cv <- train(x = don.train_NL[ , colnames(don.train_NL) != "Y"],
                   y = don.train_NL[ , colnames(don.train_NL) == "Y"],
                   method = "nnet",
                   linout = TRUE,
                   trace = FALSE,
                   maxit = 100,
                   tuneGrid = tuneMatrix,
                   trControl = controlList)


#  MODELE RETENU
# print(NN_NL_cv$finalModel)
set.seed(1)
NN_NL <- NN_NL_cv$finalModel
pred_NN_NL <- predict(NN_NL, newdata = don.test_NL)
MSE_NN_NL <- mean((pred_NN_NL - don.test_NL$Y)^2)
# MAPE_NN_NL = fun_mape(Y.test_NL, NN_NL, don.test_NL)

# tune values
NL_TV=NN_NL_cv$finalModel$tuneValue
str(NL_TV) #  size et decay 


# residus
res_NN_NL=Y.test_NL-pred_NN_NL
checkresiduals(res_NN_NL[,1]) # la distribution n'est pas vraiment gaussienne

```


# XGBOOST
```{r}

don <- base_NL_F_cr

# creation des variables Y (variable cible) 
don<- rename.variable(don, "Conso", "Y")
head(don)

# nom des variables facteurs a convertir en dummies variables
ohe_vars <- names(don)[which(sapply(don, is.factor))]

# conversion en en dummies variables
dummies <- dummyVars(~., data = don)
don_ohe <- as.data.frame(predict(dummies, newdata = don))

# remplacer les variables facteurs par les dummies
don <- cbind(don[, -c(which(colnames(don) %in% ohe_vars))], don_ohe)

# Creation de l'echantillon train et test
Xtrain <- don[train, ]
Xtest <- don[-train, ]

# modelisation
XGB_NL <- xgboost(data = data.matrix(Xtrain), label = Xtrain$Y,
  booster = "gbtree", objective = "reg:linear", eval_metric = "rmse",
  learning_rate = 0.05, 
  subsample = 0.5, seed = 1, # subsample default value=1. Setting to 0.5 means that XGBoost randomly collected half of the data instances to grow trees and this will prevent overfitting. 
  silent = 1, nrounds = 500, verbose = 0)

# prediction
pred_XGB_NL= predict(XGB_NL, data.matrix(Xtest))
MSE_XGB_NL=mean((Y.test_NL-pred_XGB_NL)^2)
# MAPE_XGB_NL = fun_mape(Y.test_NL, XGB_NL, data.matrix(Xtest))

plot(don.test_NL$Date, Y.test_NL, xlab = "date", ylab="Conso", main="XGB Pays Bas" )
lines(don.test_NL$Date,pred_XGB_NL, col="purple") 

# residus
checkresiduals(Y.test_NL-pred_XGB_NL)

```



SYNTHESE DES MODELES 


# MSE plot
```{r}

# comparaison des MSE entre les modeles 
MSE_NL_tot=c(MSE_RL_NL, MSE_RLI_NL, MSE_RLI_NL_multi, MSE_RLI_NL_P2, MSE_POLY_NL, MSE_SP_NL, MSE_GAM_NL_SP, MSE_RF_NL, MSE_SVR_NL, MSE_NN_NL, MSE_XGB_NL)

# graphe des MSE
graph<-barplot(MSE_NL_tot, xlab="modeles", ylab="MSE", main="MSE des modeles Pays Bas",las=0)
axis(1, labels=c("RL", "RLI","multi", "P2" ,"Poly" ,"SP", "GAM" ,"RF", "SVR", "NN", "XGB"), at = graph)

```

# MSE minimal
```{r}
which.min(MSE_NL_tot) # c'est le modele XGB qui presente la plus petite MSE
```

# MSE plots 5 meilleurs modeles 
```{r}

MSE_NL_r=c(MSE_RLI_NL_multi, MSE_RF_NL, MSE_SVR_NL, MSE_NN_NL, MSE_XGB_NL)

# graphe des MSE
graph<-barplot(MSE_NL_r, xlab="modeles", ylab="MSE",ylim=c(MSE_XGB_NL, MSE_RLI_NL) ,main="MSE des modeles", las=0)
axis(1, labels=c("multi", "RF", "SVR", "NN", "XGB"), at = graph)


```


objets RF, SVR, NN, XGB non reconnus par stargazer
```{r}
# on enleve RLI_P2 ainsi que POLY et Splines simples dont les MSE est plus de 2 fois supperieure a celle des autres 
stargazer(RL_NL, RLI_NL, RLI_NL_multi, RLI_NL_P2, GAM_NL_SP, type='text', flip=TRUE, title="Results", align=TRUE, column.labels = c("RL", "RLI", "multi","P2","GAM"), keep = c("Date"), model.names = TRUE, single.row = TRUE)

# le R² ajuste est le plus eleve pour RLI Multi
# le residual error est le plus faible pour RLI Multi
# F-stat est le plus eleve pour GAM

```




# graphe des valeurs predites selon les modeles
```{r}

plot(don.test_NL$Date, don.test_NL$Y, xlab = "date", ylab="Conso", main="modeles Pays Bas" )
lines(don.test_NL$Date,pred_RL_NL$fit, col="purple") 
lines(don.test_NL$Date,pred_RLI_NL$fit, col="orange") 
lines(don.test_NL$Date,pred_RLI_NL_multi$fit, col="cyan") 
lines(don.test_NL$Date,pred_POLY_NL$fit, col="pink") 
lines(don.test_NL$Date,pred_SP_NL$fit, col="green") 
lines(don.test_NL$Date,pred_GAM_NL_SP$fit, col="bisque")
lines(don.test_NL$Date, pred_RF_NL, col="red") 
lines(don.test_NL$Date, pred_SVR_NL, col="blue") 
lines(don.test_NL$Date, pred_NN_NL, col="aquamarine") 

```

```{r}
# graphes avec les 3 meilleurs modeles XGB, SVR, NN
plot(don.test_NL$Date, don.test_NL$Y, xlab = "date", ylab="Conso", main="modeles NN(aquamarine) RF (red) SVR (blue)" )
lines(don.test_NL$Date, pred_NN_NL, col="aquamarine") 
lines(don.test_NL$Date, pred_XGB_NL, col="tomato") 
lines(don.test_NL$Date, pred_SVR_NL, col="blue")



```


```{r}
plot(don.test_NL$Date, don.test_NL$Y, xlab = "date", ylab="Conso", main="modeles XGB (tomato)" )
lines(don.test_NL$Date, pred_XGB_NL, col="tomato")  

plot(don.test_NL$Date, don.test_NL$Y, xlab = "date", ylab="Conso", main="modeles SVR (blue)" )
lines(don.test_NL$Date, pred_SVR_NL, col="blue")

plot(don.test_NL$Date, don.test_NL$Y, xlab = "date", ylab="Conso", main="modeles NN(aquamarine)" )
lines(don.test_NL$Date, pred_NN_NL, col="aquamarine") 


```


residus XGB
```{r}
checkresiduals(Y.test_NL-pred_XGB_NL)


```

residus SVR
```{r}
res_SVR_NL=don.test_NL$Y-pred_SVR_NL
checkresiduals(res_SVR_NL)


```

residus reseaux neurones
```{r}
res_NN_NL=Y.test_NL-pred_NN_NL
checkresiduals(res_NN_NL[,1]) 

```



